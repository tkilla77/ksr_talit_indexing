{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Indexing\n",
    "\n",
    "*Information Retrieval* is about *selecting* the documents from a - possibly large - collection that match the *query*, and *ranking* the result set according to relevance. One obvious use-case are internet search engines, but efficient and precise search is needed in many other applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection\n",
    "\n",
    "The first part of retrieval is finding the documents that are match the query, ignoring their relative relevance for now.\n",
    "\n",
    "One obvious way to do this what we have already done for the secondary indexing case with the global city database. We created an *inverted index* or *inverted file*: a dictionary that records the set of documents (cities) that match a search term.\n",
    "\n",
    "This is the similar to the index at the back of a book, which maps index terms to pages:\n",
    "\n",
    "<img src=\"figures/book-index.png\" alt=\"Index\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {1},\n",
       " 'donut': {1, 2},\n",
       " 'on': {1},\n",
       " 'glass': {1},\n",
       " 'plate': {1},\n",
       " 'only': {2},\n",
       " 'the': {2, 3},\n",
       " 'listen': {3},\n",
       " 'to': {3},\n",
       " 'drum': {3},\n",
       " 'machine': {3}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = {\n",
    "    1: \"a donut on a glass plate\",\n",
    "    2: \"only the donut\",\n",
    "    3: \"listen to the drum machine\",\n",
    "}\n",
    "\n",
    "def extract_terms(document):\n",
    "    yield from document.split(' ')\n",
    "\n",
    "def build_full_text_index(corpus):\n",
    "    index = dict()\n",
    "    for id, document in corpus.items():\n",
    "        for term in extract_terms(document):\n",
    "            index.setdefault(term, set()).add(id)\n",
    "    return index\n",
    "\n",
    "index = build_full_text_index(documents)\n",
    "index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying\n",
    "\n",
    "We can query the index using a collection of search terms. Conveniently, we use the same term extractor on the query as we used for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2, 3}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def query_index(index, query):\n",
    "    result_set = None\n",
    "    for term in extract_terms(query):\n",
    "        results = index.get(term, set())\n",
    "        result_set = results if result_set is None else result_set.intersection(results)\n",
    "    return result_set\n",
    "\n",
    "query_index(index, \"the\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Term Extraction: Filtering & Expansion\n",
    "\n",
    "We are probably not very interested in documents containing words that appear everywhere, such as `a` and `the`. Also, we'd like our index to ignore upper- and lowercase variants.\n",
    "\n",
    "A more sophisticated term extractor would likely perform additional filtering and expansions:\n",
    "  * transform words to their stems (\"donuts\" --> \"donut\")\n",
    "  * add synonyms (\"donut\" --> \"pastry\")\n",
    "  * add acronmys (\"HTML\" --> \"hypertext markup language\")\n",
    "\n",
    "#### Excercise A\n",
    "\n",
    "Add stop-wording and case filtering to the term extraction function!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Wikipedia\n",
    "\n",
    "Can we build an index of English Wikipedia in a reasonable amount of time? Lets download the abstracts of all articles from https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract1.xml.gz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  118M  100  118M    0     0  4010k      0  0:00:30  0:00:30 --:--:-- 4020k0  3392k      0  0:00:35  0:00:05  0:00:30 3777k  0     0  3794k      0  0:00:32  0:00:09  0:00:23 4581k\n"
     ]
    }
   ],
   "source": [
    "!curl https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract1.xml.gz -o data/wiki-abstracts.xml.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at a few lines of the downloaded archive. On unix-like systems, this is straight-forward: `gzcat` streams a gzipped file to standard output, `head` limits the output to the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<feed>\n",
      "<doc>\n",
      "<title>Wikipedia: Anarchism</title>\n",
      "<url>https://en.wikipedia.org/wiki/Anarchism</url>\n",
      "<abstract>Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations.</abstract>\n",
      "<links>\n",
      "<sublink linktype=\"nav\"><anchor>Etymology, terminology, and definition</anchor><link>https://en.wikipedia.org/wiki/Anarchism#Etymology,_terminology,_and_definition</link></sublink>\n",
      "<sublink linktype=\"nav\"><anchor>History</anchor><link>https://en.wikipedia.org/wiki/Anarchism#History</link></sublink>\n",
      "<sublink linktype=\"nav\"><anchor>Pre-modern era</anchor><link>https://en.wikipedia.org/wiki/Anarchism#Pre-modern_era</link></sublink>\n",
      "<sublink linktype=\"nav\"><anchor>Modern era</anchor><link>https://en.wikipedia.org/wiki/Anarchism#Modern_era</link></sublink>\n",
      "gzcat: error writing to output: Broken pipe\n",
      "gzcat: data/wiki-abstracts.xml.gz: uncompress failed\n"
     ]
    }
   ],
   "source": [
    "!gzcat data/wiki-abstracts.xml.gz | head"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha, we are interested in the <abstract></abstract> element contents. Let's read those. Also, the article URL could serve as a useful identifier for each document. Below, we use a bit of Python XML magic to generate all (url, abstract) tuples in the downloaded archive. You don't need to understand the details (but you may well)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('https://en.wikipedia.org/wiki/Anarchism',\n",
       " 'Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations.')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_wikipedia_abstracts(filename):\n",
    "    \"\"\"Read the given filename and yield each articles fulltext.\"\"\"\n",
    "    from io import TextIOWrapper\n",
    "    import gzip\n",
    "    from xml.dom import pulldom\n",
    "\n",
    "    with gzip.open(f'data/{filename}.gz', mode='rt') as xml:\n",
    "        doc = pulldom.parse(xml)\n",
    "        url = None\n",
    "        abstract = None\n",
    "        for event, node in doc:\n",
    "            try:\n",
    "                if event == pulldom.START_ELEMENT and node.tagName == 'url':\n",
    "                    doc.expandNode(node)\n",
    "                    url = node.firstChild.data\n",
    "                elif event == pulldom.START_ELEMENT and node.tagName == 'abstract':\n",
    "                    doc.expandNode(node)\n",
    "                    abstract = node.firstChild.data\n",
    "                    yield url, abstract\n",
    "            except Exception as e:\n",
    "                print(f'Error around {url}: {e}')\n",
    "\n",
    "next(read_wikipedia_abstracts(\"wiki-abstracts.xml\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 2\n",
    "\n",
    "Create an index of all wikipedia abstracts!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
